{"cells": [{"cell_type": "code", "execution_count": 9, "id": "decf1656-67e4-4366-b421-ef2a6199b121", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Scala language: version 2.12.18\n(49,2499643)\n"}], "source": "import java.time.LocalTime\nimport org.apache.spark.sql.SparkSession\n\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.expressions.Window\nimport org.apache.spark.sql.functions.udf\nimport org.apache.spark.sql.DataFrame\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.functions.col\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.functions.lag\nimport org.apache.spark.sql.expressions.Window\n\nimport org.apache.spark.mllib.tree.model.DecisionTreeModel\nimport org.apache.spark.mllib.tree.DecisionTree\nimport org.apache.spark.mllib.util.MLUtils\nimport org.apache.spark.mllib.evaluation.MulticlassMetrics \nimport org.apache.spark.mllib.evaluation.BinaryClassificationMetrics  \n\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.classification.{RandomForestClassificationModel, RandomForestClassifier}\nimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\nimport org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer, VectorAssembler}\n\n\nval spark = SparkSession\n  .builder()\n  .appName(\"Spark ML\")\n  .config(\"spark.some.config.option\", \"some-value\")\n  .getOrCreate()\n\nimport spark.sqlContext.implicits._\n\nval bucket = \"dataproc-temp-us-central1-1044206227610-i54vpwyj\"\nspark.conf.set(\"temporaryGcsBucket\", bucket)\n\nprintln(\"Scala language: \"+util.Properties.versionString)\n\n// spark.sparkContext.version\nspark.version\n\nval data_raw = spark.read.parquet(\"gs://dataset-flight/final-for-ml.parquet\")\n//     .drop(\"ARR_DATETIME_RND\")\n//     .drop(\"ARR_WBAN\")\n\nval data = data_raw\n    .drop(data_raw.columns.filter(colName => colName.contains(\"2\")) : _*)\n    .drop(data_raw.columns.filter(colName => colName.contains(\"3\")) : _*)\n    .drop(data_raw.columns.filter(colName => colName.contains(\"WindSpeed\")) : _*)\n    .na.drop()\n\nval nbRows = data.count()\nprintln(data.columns.size, nbRows)"}, {"cell_type": "code", "execution_count": 10, "id": "be7dd976-8d90-43f3-81b9-1d9963ddc69f", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-------+--------------------+\n|summary|          IS_DELAYED|\n+-------+--------------------+\n|  count|             2499643|\n|   mean|0.029532617257744406|\n+-------+--------------------+\n\n"}], "source": "data.select(\"IS_DELAYED\").summary(\"count\", \"mean\").show()"}, {"cell_type": "code", "execution_count": 11, "id": "d4e10603-3991-4e7a-8e40-c017f6ddede4", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "(49,48)\n"}, {"data": {"text/plain": "featColumns = Array(DEP_DryBulbFarenheit, DEP_Visibility, DEP_WindDirection, DEP_StationPressure, DEP_WetBulbFarenheit, DEP_DewPointCelsius, DEP_clearSky, DEP_fewClouds, DEP_scatterClouds, DEP_brokenClouds, DEP_overCast, DEP_obscuredSky, DEP_partiallyObscuredSky, DEP_WindDirection1, DEP_StationPressure1, DEP_WetBulbFarenheit1, DEP_DewPointCelsius1, DEP_clearSky1, DEP_fewClouds1, DEP_scatterClouds1, DEP_brokenClouds1, DEP_overCast1, DEP_obscuredSky1, DEP_partiallyObscuredSky1, ARR_DryBulbFarenheit, ARR_Visibility, ARR_WindDirection, ARR_StationPressure, ARR_WetBulbFarenheit, ARR_DewPointCelsius, ARR_clearSky, ARR_fewClouds, ARR_scatterClouds, ARR_brokenClouds, ARR_overCast, ARR_obscuredSky, ARR_partiallyObscuredSky, ARR_WindDirection1, ARR_StationPressure1, ARR_WetBulbFare...\n"}, "metadata": {}, "output_type": "display_data", "source": "user"}, {"data": {"text/plain": "Array(DEP_DryBulbFarenheit, DEP_Visibility, DEP_WindDirection, DEP_StationPressure, DEP_WetBulbFarenheit, DEP_DewPointCelsius, DEP_clearSky, DEP_fewClouds, DEP_scatterClouds, DEP_brokenClouds, DEP_overCast, DEP_obscuredSky, DEP_partiallyObscuredSky, DEP_WindDirection1, DEP_StationPressure1, DEP_WetBulbFarenheit1, DEP_DewPointCelsius1, DEP_clearSky1, DEP_fewClouds1, DEP_scatterClouds1, DEP_brokenClouds1, DEP_overCast1, DEP_obscuredSky1, DEP_partiallyObscuredSky1, ARR_DryBulbFarenheit, ARR_Visibility, ARR_WindDirection, ARR_StationPressure, ARR_WetBulbFarenheit, ARR_DewPointCelsius, ARR_clearSky, ARR_fewClouds, ARR_scatterClouds, ARR_brokenClouds, ARR_overCast, ARR_obscuredSky, ARR_partiallyObscuredSky, ARR_WindDirection1, ARR_StationPressure1, ARR_WetBulbFare..."}, "execution_count": 11, "metadata": {}, "output_type": "execute_result"}], "source": "// columns that need to added to feature column\nval featColumns = data.columns.filter(name => !name.contains(\"IS_DELAYED\"))\nprintln(data.columns.size, featColumns.length)\n\n// VectorAssembler to add feature column\n// input columns - cols\n// feature column - features\nval vectorAssembler = new VectorAssembler()\n    .setInputCols(featColumns)\n    .setOutputCol(\"features\")\n    .setHandleInvalid(\"skip\") // options are \"keep\", \"error\" or \"skip\"\n\n\nval featureDf = vectorAssembler.transform(data)\n// featureDf.printSchema()\n\n\n// StringIndexer define new 'label' column with 'result' column\n// Index labels, adding metadata to the label column.\n// Fit on whole dataset to include all labels in index.\nval labelIndexer = new StringIndexer()\n  .setInputCol(\"IS_DELAYED\")\n  .setOutputCol(\"label\")\n\n\nval labelDf = labelIndexer.fit(featureDf).transform(featureDf)\n// labelDf.printSchema()"}, {"cell_type": "code", "execution_count": 12, "id": "6655df94-e581-4868-95c2-3278dce2272d", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "There are 221701 rows in the training set, and 94543 in the test set\n"}, {"data": {"text/plain": "seed = 42\nzeros = [IS_DELAYED: bigint, DEP_DryBulbFarenheit: bigint ... 49 more fields]\nones = [IS_DELAYED: bigint, DEP_DryBulbFarenheit: bigint ... 49 more fields]\ntrain0 = [IS_DELAYED: bigint, DEP_DryBulbFarenheit: bigint ... 49 more fields]\ntest0 = [IS_DELAYED: bigint, DEP_DryBulbFarenheit: bigint ... 49 more fields]\ntrain1 = [IS_DELAYED: bigint, DEP_DryBulbFarenheit: bigint ... 49 more fields]\ntest1 = [IS_DELAYED: bigint, DEP_DryBulbFarenheit: bigi...\n"}, "metadata": {}, "output_type": "display_data", "source": "user"}, {"data": {"text/plain": "[IS_DELAYED: bigint, DEP_DryBulbFarenheit: bigi..."}, "execution_count": 12, "metadata": {}, "output_type": "execute_result"}], "source": "// Split the data into training and test sets (30% held out for testing).\n// split data set training and test\n// training data set - 70%\n// test data set - 30%\nval seed = 42\n\nval zeros = labelDf.filter(col(\"IS_DELAYED\") === 0).sample(0.1, seed)\nval ones = labelDf.filter(col(\"IS_DELAYED\") === 1)\n\n// split datasets into training and testing\nval Array(train0, test0) = zeros.randomSplit(Array(0.7, 0.3), seed)\nval Array(train1, test1) = ones.randomSplit(Array(0.7, 0.3), seed)\n\n// stack datasets back together\nval trainingData = train0.union(train1)\nval testData = test0.union(test1)\n\nprintln(f\"\"\"There are ${trainingData.count} rows in the training set, and ${testData.count} in the test set\"\"\")"}, {"cell_type": "code", "execution_count": 13, "id": "fe14f739-9e79-4fae-a624-d2bf122a2f72", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-------+-------------------+\n|summary|         IS_DELAYED|\n+-------+-------------------+\n|  count|              94543|\n|   mean|0.23189448187597178|\n+-------+-------------------+\n\n"}], "source": "testData.select(\"IS_DELAYED\").summary(\"count\", \"mean\").show()"}, {"cell_type": "code", "execution_count": 14, "id": "bb15407e-3d27-4121-a2fb-b6dc33a5af55", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-------+------------------+\n|summary|        IS_DELAYED|\n+-------+------------------+\n|  count|            221701|\n|   mean|0.2340855476520178|\n+-------+------------------+\n\n"}], "source": "trainingData.select(\"IS_DELAYED\").summary(\"count\", \"mean\").show()"}, {"cell_type": "code", "execution_count": 15, "id": "32832d6f-cc22-4723-83db-db4cc32f421c", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------+--------------------+-----+--------------------+--------------------+----------+\n|IS_DELAYED|            features|label|       rawPrediction|         probability|prediction|\n+----------+--------------------+-----+--------------------+--------------------+----------+\n|         0|(48,[0,1,2,3,4,5,...|  0.0|[24.4463421293850...|[0.81487807097950...|       0.0|\n|         0|(48,[0,1,3,4,5,7,...|  0.0|[24.1233321464375...|[0.80411107154791...|       0.0|\n|         0|(48,[0,1,3,4,5,9,...|  0.0|[25.3166075611762...|[0.84388691870587...|       0.0|\n|         0|(48,[0,1,2,3,4,5,...|  0.0|[27.9958769651046...|[0.93319589883682...|       0.0|\n|         0|(48,[0,1,3,4,5,7,...|  0.0|[26.4653602134310...|[0.88217867378103...|       0.0|\n+----------+--------------------+-----+--------------------+--------------------+----------+\nonly showing top 5 rows\n\n"}, {"data": {"text/plain": "randomForestClassifier = rfc_e217715724d7\nrandomForestModel = RandomForestClassificationModel: uid=rfc_e217715724d7, numTrees=30, numClasses=2, numFeatures=48\npredictionDf = [IS_DELAYED: bigint, DEP_DryBulbFarenheit: bigint ... 52 more fields]\n"}, "metadata": {}, "output_type": "display_data", "source": "user"}, {"data": {"text/plain": "[IS_DELAYED: bigint, DEP_DryBulbFarenheit: bigint ... 52 more fields]"}, "execution_count": 15, "metadata": {}, "output_type": "execute_result"}], "source": "// train Random Forest model with training data set\nval randomForestClassifier = new RandomForestClassifier()\n  .setImpurity(\"gini\")\n  .setMaxDepth(15)\n  .setNumTrees(30)\n  .setFeatureSubsetStrategy(\"auto\")\n  .setSeed(seed)\n  .setLabelCol(\"label\") // indexedLabel\n  .setFeaturesCol(\"features\") // indexedFeatures\n\nval randomForestModel = randomForestClassifier.fit(trainingData)\n\nval predictionDf = randomForestModel.transform(testData)\n\n// Select example rows to display.\npredictionDf.select(\"IS_DELAYED\", \"features\", \"label\", \"rawPrediction\", \"probability\", \"prediction\").show(5)"}, {"cell_type": "markdown", "id": "09695395-3b31-482f-89fc-164fb30ba3e0", "metadata": {}, "source": "Confusion matrix"}, {"cell_type": "code", "execution_count": 16, "id": "47c8f450-5aa4-4a1e-ba71-b4ee832ea95e", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Confusion matrix:\n70945.0  1674.0  \n16735.0  5189.0  \n"}, {"data": {"text/plain": "predictionAndLabels = MapPartitionsRDD[269] at rdd at <console>:113\nbMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@5f9519f7\nmMetrics = org.apache.spark.mllib.evaluation.MulticlassMetrics@371c5d51\nlabels = Array(0.0, 1.0)\ntrueNegative = 70945.0\ntruePositive = 5189.0\nfalseNegative = 16735.0\nfalsePositive = 1674.0\ntotalDelays = 21924\nprecision = 0.7560833454757395\nrecall = 0.23668126254333152\nacc = 0.8052843679595528\nf1score = 0.3605099524090735\n"}, "metadata": {}, "output_type": "display_data", "source": "user"}, {"data": {"text/plain": "0.3605099524090735"}, "execution_count": 16, "metadata": {}, "output_type": "execute_result"}], "source": "val predictionAndLabels = predictionDf\n    .select(\"prediction\", \"label\")\n    .as[(Double, Double)]\n    .rdd\n\n// Instantiate a new metrics objects\nval bMetrics = new BinaryClassificationMetrics(predictionAndLabels)\nval mMetrics = new MulticlassMetrics(predictionAndLabels)\nval labels = mMetrics.labels\n\n// Print out the Confusion matrix\nprintln(\"Confusion matrix:\")\nprintln(mMetrics.confusionMatrix)\n\nval trueNegative = predictionDf.filter(col(\"prediction\") === 0 && col(\"label\") === col(\"prediction\")).count().toDouble\nval truePositive = predictionDf.filter(col(\"prediction\") === 1 && col(\"label\") === col(\"prediction\")).count().toDouble\nval falseNegative = predictionDf.filter(col(\"prediction\") === 0 && col(\"label\") =!= col(\"prediction\")).count().toDouble\nval falsePositive = predictionDf.filter(col(\"prediction\") === 1 && col(\"label\") =!= col(\"prediction\")).count().toDouble\n\nval totalDelays = predictionDf.filter(col(\"label\") === 1).count()\n\nval precision = truePositive / (truePositive + falsePositive) \nval recall = truePositive / (truePositive + falseNegative) \nval acc = (truePositive + trueNegative) / (truePositive + trueNegative + falsePositive + falseNegative) \nval f1score = 2 * precision * recall / (precision + recall) "}, {"cell_type": "markdown", "id": "adcb42a3-f6d4-4c5a-abe3-4864fa2362e7", "metadata": {}, "source": "## Conclusion:\n- nulls intelligement + FEAT selection on enl\u00e8ve _2 & _3\n- stratified split + undersampling\n- Random Forest MaxDepth(__15__)  .setNumTrees(__30__)\n- precision = 0.75\n- recall = 0.24\n- acc = 0.81\n- f1score = 0.36"}, {"cell_type": "markdown", "id": "d4e184fa-ebfa-4b8b-a873-f5946d2704ab", "metadata": {}, "source": "feature importance"}, {"cell_type": "markdown", "id": "5b3aa558-e895-4d47-b2d5-eeb617263a20", "metadata": {}, "source": "https://stackoverflow.com/questions/47043836/scala-random-forest-feature-importance-extraction-with-names-labels"}, {"cell_type": "code", "execution_count": 26, "id": "2f2026a5-45dd-490f-85df-2bd0af5eeecf", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "(DEP_DryBulbFarenheit,0.09382043746894786)\n(DEP_WindSpeed,0.04370395003943582)\n(DEP_WindDirection,0.03871905883838827)\n(ARR_DryBulbFarenheit,0.038422662389145906)\n(DEP_StationPressure,0.036825118940613154)\n(DEP_WindSpeed1,0.028278354339229934)\n(ARR_WindSpeed,0.024823332949721562)\n(DEP_Visibility,0.024500251206569505)\n(ARR_WindDirection3,0.0237414653741213)\n(DEP_WindDirection1,0.02215922532080796)\n(ARR_WindDirection,0.021235844920245953)\n(DEP_WindSpeed2,0.020899678261048275)\n(ARR_DryBulbFarenheit2,0.020110689751338978)\n(ARR_WindSpeed1,0.019921161509000935)\n(ARR_Visibility,0.019465331837996484)\n(ARR_WindSpeed3,0.0184810343380359)\n(ARR_StationPressure,0.018194620859103575)\n(ARR_WindSpeed2,0.01629911240552861)\n(ARR_WindDirection1,0.015676205758860497)\n(ARR_WindDirection2,0.01559791315197842)\n"}, {"data": {"text/plain": "featureImportance = (108,[0,1,2,3,4,5,6,7,8,9,10,11,14,15,16,17,18,19,20,21,22,23,24,26,27,28,29,30,31,32,33,34,35,36,37,38,40,41,42,43,44,45,46,47,48,49,50,51,54,55,56,57,58,59,60,61,62,63,64,65,68,69,70,71,72,73,74,75,76,77,78,80,81,82,83,84,85,86,88,89,90,91,92,94,95,96,97,98,99,100,101,102,103,104,105,106],[0.09382043746894786,0.024500251206569505,0.03871905883838827,0.04370395003943582,0.036825118940613154,0.01062246343702173,0.007872779224264983,9.380841791536837E-4,0.005963770656519379,0.0037234492316601717,0.008461155404203068,0.014366613387173618,0.02215922532080796,0.028278354339229934,0.014111237875049462,0.013685486108309995,0.010640734604133248,7.917591873401629E-4,0.003089402446096833,0.002670016894893009,0.006981975850576899,0.0037139099...\n"}, "metadata": {}, "output_type": "display_data", "source": "user"}, {"data": {"text/plain": "(108,[0,1,2,3,4,5,6,7,8,9,10,11,14,15,16,17,18,19,20,21,22,23,24,26,27,28,29,30,31,32,33,34,35,36,37,38,40,41,42,43,44,45,46,47,48,49,50,51,54,55,56,57,58,59,60,61,62,63,64,65,68,69,70,71,72,73,74,75,76,77,78,80,81,82,83,84,85,86,88,89,90,91,92,94,95,96,97,98,99,100,101,102,103,104,105,106],[0.09382043746894786,0.024500251206569505,0.03871905883838827,0.04370395003943582,0.036825118940613154,0.01062246343702173,0.007872779224264983,9.380841791536837E-4,0.005963770656519379,0.0037234492316601717,0.008461155404203068,0.014366613387173618,0.02215922532080796,0.028278354339229934,0.014111237875049462,0.013685486108309995,0.010640734604133248,7.917591873401629E-4,0.003089402446096833,0.002670016894893009,0.006981975850576899,0.0037139099..."}, "execution_count": 26, "metadata": {}, "output_type": "execute_result"}], "source": "val featureImportance = decisionTreeModel.featureImportances\n\n// featureImportance.getClass\n\nval res = featColumns.zip(featureImportance.toArray).sortBy(-_._2)\n\n\nfor (e <- res.take(20)) println(e)"}], "metadata": {"kernelspec": {"display_name": "Apache Toree - Scala", "language": "scala", "name": "apache_toree_scala"}, "language_info": {"codemirror_mode": "text/x-scala", "file_extension": ".scala", "mimetype": "text/x-scala", "name": "scala", "pygments_lexer": "scala", "version": "2.12.15"}}, "nbformat": 4, "nbformat_minor": 5}